# AI-EnergyR5
AI-Driven Predictive Maintenance for Renewable Energy Assets 
# AI-Driven Predictive Maintenance for Renewable Energy Assets

This project develops a cross-platform application for predictive maintenance of renewable energy assets (wind turbines, solar panels, inverters, batteries). It uses IoT sensor data, external weather/solar APIs, and AI/ML models to forecast failures and optimize maintenance schedules.

---

## ğŸš€ Features
- Real-time sensor data ingestion (temperature, humidity, irradiance, wind speed).
- External API integration (OpenWeather, NASA POWER, Tomorrow.io).
- Local PostgreSQL + TimescaleDB storage for time-series data.
- Preprocessing scripts for normalization, cleaning, and interpolation.
- Ready for deployment on Raspberry Pi 4, but fully compatible with Mac and Windows laptops during development.

---

## ğŸ› ï¸ Development Setup

### 1. Clone Repository
```bash
AI-EnergyR5/
â”‚
â”œâ”€â”€ README.md             # Documentation for setup and usage
â”œâ”€â”€ config.py             # Configuration settings (API keys, database credentials)
â”œâ”€â”€ requirements.txt      # List of Python dependencies
â”‚
â”œâ”€â”€ api_wrappers/         # External API integration modules
â”‚   â”œâ”€â”€ openweather.py    # OpenWeather API wrapper for weather data
â”‚   â””â”€â”€ nasa_power.py     # NASA POWER API wrapper for solar irradiance data
â”‚
â”œâ”€â”€ data/                 # Data files and logs
â”‚   â”œâ”€â”€ sensor_logs.txt   # Plain text sensor log file
â”‚   â””â”€â”€ sensor_data.csv   # CSV file for sensor data
â”‚
â”œâ”€â”€ db/                   # Database setup and connectors
â”‚   â”œâ”€â”€ db_connector.py   # Python script for DB connection
â”‚   â”œâ”€â”€ db_ingest.py      # Data ingestion script
â”‚   â”œâ”€â”€ test_connection.py # Quick connection test script
â”‚   â”œâ”€â”€ schema.sql        # SQL table definitions
â”‚   â”œâ”€â”€ sensor_stream_sim.py # Sensor stream simulation
â”‚   â””â”€â”€ api_ingest_openweather.py # OpenWeather API ingestion
â”‚
â”œâ”€â”€ docs/                 # Documentation and notes
â”‚   â”œâ”€â”€ myNotes.txt       # Development notes and progress logs
â”‚   â””â”€â”€ TODO.md           # Task list and project roadmap
â”‚
â”œâ”€â”€ logs/                 # Log files
â”‚   â”œâ”€â”€ ingestion.log     # Today's ingestion log
â”‚   â”œâ”€â”€ ingestion.log.2026-01-20 # Yesterday's log (auto-rotated)
â”‚   â””â”€â”€ ingestion.log.2026-01-26 # Older log (auto-rotated)
â”‚
â”œâ”€â”€ notebooks/            # Jupyter notebooks for demos
â”‚   â””â”€â”€ data_pipeline_demo.py # Step-by-step interactive demo
â”‚
â”œâ”€â”€ preprocessing/        # Data cleaning and preprocessing scripts
â”‚   â””â”€â”€ preprocess.py     # Normalize and clean sensor logs
â”‚
â”œâ”€â”€ scripts/              # Utility scripts
â”‚   â”œâ”€â”€ capture_weather_data.py # Automated weather data capture
â”‚   â”œâ”€â”€ show_recent_data.py     # Display recent sensor data
â”‚   â””â”€â”€ run_ingest.bat          # Batch file for scheduled ingestion
â”‚
â”œâ”€â”€ sensors/              # Sensor data scripts
â”‚   â””â”€â”€ sensor_ingest.py  # Generate or simulate sensor readings
â”‚
â”œâ”€â”€ tests/                # Testing and validation scripts
â”‚   â”œâ”€â”€ check_schema.py   # Schema validation
â”‚   â””â”€â”€ test_imports.py   # Import testing
â”‚
â””â”€â”€ web/                  # Web-related files
    â”œâ”€â”€ dashboard.py      # Streamlit dashboard
    â”œâ”€â”€ generate_html_table.py # HTML table generation
    â”œâ”€â”€ ingestion_trigger.py   # Flask endpoint for ingestion
    â””â”€â”€ solar_wind_display.html # HTML interface for data display
```

### 2. PostgreSQL Database Management

This project uses PostgreSQL as the database backend. Follow these steps to turn PostgreSQL on and off:

#### Turn PostgreSQL On (Start the Server)
1. **Open Command Prompt Window**:
   - Press `Win + R`, type `cmd`, and press Enter
   - Or search for "Command Prompt" in the Start menu

2. **Navigate to PostgreSQL Bin Directory**:
   - In the Command Prompt window, type the following command and press Enter:
     ```
     cd "D:\My Documents\tools\postgresql\pgsql\bin"
     ```

3. **Start PostgreSQL Server**:
   - In the same Command Prompt window, type the following command and press Enter:
     ```
     pg_ctl.exe -D "D:\My Documents\tools\postgresql\pgsql\data" -l logfile start
     ```
   - This starts PostgreSQL in the background on port 5432
   - You should see a message indicating the server is starting
   - The server will continue running until manually stopped

4. **Verify PostgreSQL is Running** (Optional):
   - In the same Command Prompt window, type the following command and press Enter:
     ```
     pg_ctl.exe -D "D:\My Documents\tools\postgresql\pgsql\data" status
     ```
   - Should show: "pg_ctl: server is running (PID: XXXX)"

#### Turn PostgreSQL Off (Stop the Server)
1. **Open Command Prompt Window**:
   - Press `Win + R`, type `cmd`, and press Enter
   - Or search for "Command Prompt" in the Start menu

2. **Navigate to PostgreSQL Bin Directory**:
   - In the Command Prompt window, type the following command and press Enter:
     ```
     cd "D:\My Documents\tools\postgresql\pgsql\bin"
     ```

3. **Stop PostgreSQL Server**:
   - In the same Command Prompt window, type the following command and press Enter:
     ```
     pg_ctl.exe -D "D:\My Documents\tools\postgresql\pgsql\data" stop
     ```
   - This performs a clean shutdown of the database server
   - You should see a message indicating the server is stopping

#### Notes
- PostgreSQL must be running before you can connect to the database from Python scripts
- The database connection settings are configured in `db/db_connector.py` with default values:
  - Host: `localhost`
  - Port: `5432`
  - Database: `energy_db`
  - User: `postgres`
  - Password: `PdM`
- To test the database connection, run: `python db/test_connection.py`

#### Notes

For detailed development notes and progress logs, refer to `mynotes.txt`.

---

## ğŸ“‹ Project Phases

The project is organized into phases for systematic development. Below is the latest status of all phases with detailed sub-steps:

### Phase 1: Environment Setup âœ… Done
- Install PostgreSQL portable binaries
- Initialize database cluster (initdb)
- Start PostgreSQL manually (pg_ctl)
- Connect with psql

### Phase 2: Database Schema âœ… Done
- Create energy_db database
- Define sensor_data table schema
- Verify schema with \d sensor_data

### Phase 3: Python Integration âœ… Done
- Install psycopg2 driver
- Create db_ingest.py script
- Connect Python to PostgreSQL
- Insert test row via Python
- Fetch and display rows via Python

### Phase 4: Log Ingestion âœ… Done
- Adapt script to read sensor_logs.txt
- Insert multiple rows from file
- Verify ingestion with query output

### Phase 5: Enhancements âœ… Done
- Handle duplicate entries (unique timestamp + ON CONFLICT)
- Format timestamp output (seconds only)
- Optional: pretty table output
- Row count before/after ingestion
- Skip header line in text ingestion
- Modularize connection into db_connector.py
- Add test_connection.py script
- Show top/bottom rows in test script

### Phase 6: Next Steps âœ… Done
- Automate ingestion (batch file or cron job)
- Extend ingestion for CSV/real sensor streams
- Dashboard/visualization integration
- Add permanent log file output (logs/ingestion.log)
- Daily log rotation (TimedRotatingFileHandler)

### Phase 7: Visualization & Dashboard âœ… Done
- Plot temperature vs timestamp chart
- Add multiple charts (humidity, irradiance, wind speed)
- Build simple dashboard (Streamlit with sidebar)

### Phase 8: Real-Time Ingestion ğŸ”„ Partial
- Simulate sensor streams (append rows every minute) âœ… Done
- Implement manual trigger for on-demand ingestion âœ… Done
- Enable continuous ingestion pipeline â³ Pending

### Phase 9: Predictive Analytics â³ Pending
- Calculate averages/min/max/moving averages
- Train ML model for forecasting (scikit-learn)

### Phase 10: Deployment & Scaling â³ Pending
- Containerize with Docker
- Deploy to cloud (AWS/Azure/GCP)

### Phase 11: Web-Sensor Data Integration ğŸ”„ Partial
- Connect to OpenWeather API for local weather data âœ… Done
- Ingest NASA POWER API for solar irradiance and climate data âœ… Done
- Integrate PVOutput API for solar PV system performance â³ Pending
- Optional: Add other APIs (NOAA, Meteostat, etc.) â³ Pending
- Normalize and store web-sensor data into sensor_data table âœ… Done
- Combine local sensor + web API data for richer analytics â³ Pending
